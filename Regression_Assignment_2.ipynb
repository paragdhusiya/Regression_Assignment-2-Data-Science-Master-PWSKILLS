{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression_Assignment - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "R-squared (R2) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model.\n",
    "It is calculated as the ratio of the explained variance to the total variance of the dependent variable.\n",
    "R2 ranges from 0 to 1, where 0 indicates that the independent variables do not explain any variability in the dependent variable, and 1 indicates that they explain all the variability.\n",
    "Mathematically, R2 is calculated as: R2 = 1 - (SSres / SStot), where SSres is the sum of squared residuals and SStot is the total sum of squares.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adjusted R-squared (R2_adj) is a modified version of R-squared that penalizes the addition of unnecessary independent variables to the regression model.\n",
    "Unlike R-squared, which always increases as more independent variables are added, R2_adj increases only if the additional variables improve the model more than would be expected by chance.\n",
    "R2_adj is calculated as: R2_adj = 1 - [(1 - R2) * (n - 1) / (n - k - 1)], where n is the number of observations and k is the number of independent variables.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of independent variables.\n",
    "It helps to avoid the problem of overfitting by penalizing the addition of unnecessary variables and provides \n",
    "a more accurate measure of the goodness of fit.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics to evaluate the performance of regression models.\n",
    "RMSE is the square root of the average of the squared differences between the predicted and actual values.\n",
    "MSE is the average of the squared differences between the predicted and actual values.\n",
    "MAE is the average of the absolute differences between the predicted and actual values.\n",
    "These metrics represent the average error between the predicted and actual values, with lower values indicating better model performance.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages:\n",
    "# - RMSE and MSE penalize larger errors more heavily, making them sensitive to outliers.\n",
    "# - MAE is more robust to outliers since it uses absolute differences.\n",
    "# Disadvantages:\n",
    "# - RMSE and MSE are sensitive to the scale of the dependent variable.\n",
    "# - MAE does not differentiate between the magnitude of errors.\n",
    "# - RMSE and MSE are affected by the presence of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used to add a penalty term to the regression model's cost function to prevent overfitting.\n",
    "It differs from Ridge regularization in that it adds the absolute value of the coefficients as the penalty term, leading to sparse solutions where some coefficients become exactly zero.\n",
    "Lasso is more appropriate when there is a need for feature selection or when dealing with a large number of features, as it can automatically perform variable selection by setting some coefficients to zero.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regularized linear models add penalty terms to the cost function, which discourages overly complex models with large coefficients.\n",
    "This helps to prevent overfitting by penalizing high coefficients and reducing the model's sensitivity to noisy or irrelevant features.\n",
    "For example, in Ridge regression, the penalty term is the squared sum of the coefficients multiplied by a regularization parameter lambda (Î»), which controls the strength of regularization.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regularized linear models assume that all features are equally important, which may not be true in practice.\n",
    "They also require tuning of hyperparameters such as the regularization parameter, which can be challenging.\n",
    "Additionally, regularized models may not perform well if the relationship between the features and the target variable is highly nonlinear.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The choice between RMSE and MAE depends on the specific characteristics of the problem and the importance of different types of errors.\n",
    "RMSE penalizes larger errors more heavily, so if the presence of outliers is a concern, it may be preferred.\n",
    "However, if outliers are less of a concern and you want a more robust metric, MAE may be a better choice.\n",
    "Therefore, without further context, it is difficult to determine which model is better solely based on these metrics.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The choice between Ridge and Lasso regularization depends on the specific characteristics of the problem and the importance of feature selection.\n",
    "Ridge regularization tends to shrink all coefficients towards zero, while Lasso regularization can lead to sparsity by setting some coefficients exactly to zero.\n",
    "If feature selection is important and there is a need to reduce the number of features in the model, Lasso regularization may be preferred.\n",
    "However, if interpretability of the coefficients is not a concern and the goal is simply to reduce overfitting, Ridge regularization may be a better choice.\n",
    "Therefore, the choice between the two regularization methods depends on the trade-offs between model complexity, interpretability, and predictive performance.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
